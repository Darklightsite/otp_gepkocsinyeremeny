"""Adatkezel≈ë a koordin√°ci√≥hoz."""
import logging
import re
import json
import os
import aiohttp
import async_timeout
import asyncio
from datetime import timedelta, datetime

from homeassistant.helpers.update_coordinator import DataUpdateCoordinator, UpdateFailed
from homeassistant.components import persistent_notification
from .const import DOMAIN, CONF_NUMBERS

_LOGGER = logging.getLogger(__name__)

SCAN_INTERVAL = timedelta(hours=12)
URL = "https://www.otpbank.hu/portal/hu/megtakaritas/forint-betetek/gepkocsinyeremeny"

class OTPCoordinator(DataUpdateCoordinator):
    """Adatok kezel√©se √©s friss√≠t√©se."""

    def __init__(self, hass, numbers_str):
        """Inicializ√°l√°s."""
        super().__init__(
            hass,
            _LOGGER,
            name="OTP G√©pkocsinyerem√©ny",
            update_interval=SCAN_INTERVAL,
        )
        self.hass = hass
        
        # Bet√©tsz√°mok tiszt√≠t√°sa
        self.my_numbers = []
        if numbers_str:
            raw_nums = numbers_str.replace(",", " ").split()
            for num in raw_nums:
                clean_num = re.sub(r"[^0-9]", "", num)
                if len(clean_num) > 0:
                    self.my_numbers.append(clean_num)
        
        _LOGGER.debug(f"Figyelt bet√©tek: {self.my_numbers}")

        self._state_file = hass.config.path("otp_gepkocsi_state.json")
        self._history_file = hass.config.path("otp_nyeremeny_history.json")
        self._all_winners_file = hass.config.path("otp_all_winners.json")
        
        self.data = {
            "nyeremenyek": 0,
            "nyertes_reszletek": [],
            "utolso_sorsolas": "Ismeretlen",
            "kovetkezo_sorsolas": "Ismeretlen",
            "nyeremeny_tortenelem": [],
            "figyelt_db": len(self.my_numbers)
        }
        
        self._history = []
        self._checked_pdfs = []
        self._all_winners = {}

    async def _extract_text_from_pdf(self, session, url):
        """Let√∂lti √©s kinyeri a sz√∂veget egy PDF-b≈ël pypdf seg√≠ts√©g√©vel."""
        try:
            async with session.get(url, timeout=aiohttp.ClientTimeout(total=60)) as response:
                if response.status != 200:
                    _LOGGER.debug(f"PDF nem el√©rhet≈ë ({response.status}): {url}")
                    return None
                pdf_bytes = await response.read()
            
            text = ""
            try:
                def parse_pdf():
                    import io
                    from pypdf import PdfReader
                    f = io.BytesIO(pdf_bytes)
                    reader = PdfReader(f)
                    extracted = ""
                    for page in reader.pages:
                        extracted += page.extract_text() + "\n"
                    return extracted

                text = await self.hass.async_add_executor_job(parse_pdf)
                
            except ImportError:
                _LOGGER.error("A pypdf k√∂nyvt√°r nem tal√°lhat√≥!")
                text = pdf_bytes.decode('latin-1', errors='ignore')
            except Exception as e:
                _LOGGER.debug(f"PDF feldolgoz√°si hiba (pypdf): {e}")
                text = pdf_bytes.decode('latin-1', errors='ignore')
            
            return text
        except asyncio.TimeoutError:
            _LOGGER.debug(f"PDF let√∂lt√©si timeout: {url}")
            return None
        except Exception as e:
            _LOGGER.debug(f"PDF let√∂lt√©si hiba ({url}): {e}")
            return None

    def _extract_pdf_urls_from_html(self, html_content):
        """Kinyeri a PDF URL-eket az OTP oldalb√≥l."""
        pattern = r'(?:https://www\.otpbank\.hu)?/static/portal/sw/file/GK_\d{8}(?:_extra)?\.pdf'
        urls = re.findall(pattern, html_content)
        
        seen = set()
        unique_urls = []
        for url in urls:
            if url.startswith("/"):
                url = f"https://www.otpbank.hu{url}"
            if url not in seen:
                seen.add(url)
                unique_urls.append(url)
        
        # Fallback: generate URLs for recent months (may not be linked yet)
        from datetime import datetime, timedelta
        base_url = "https://www.otpbank.hu/static/portal/sw/file/GK_{}.pdf"
        today = datetime.now()
        for months_ago in range(3):  # Check last 3 months
            check_date = today - timedelta(days=months_ago * 30)
            # Try 15th of month
            date_str = check_date.strftime("%Y%m") + "15"
            fallback_url = base_url.format(date_str)
            if fallback_url not in seen:
                seen.add(fallback_url)
                unique_urls.append(fallback_url)
            # Try 17th of month (sometimes used)
            date_str17 = check_date.strftime("%Y%m") + "17"
            fallback_url17 = base_url.format(date_str17)
            if fallback_url17 not in seen:
                seen.add(fallback_url17)
                unique_urls.append(fallback_url17)
        
        return unique_urls

    def _parse_date_from_pdf_url(self, url):
        """Kinyeri a d√°tumot a PDF URL-b≈ël."""
        match = re.search(r'GK_(\d{4})(\d{2})(\d{2})', url)
        if match:
            return f"{match.group(1)}. {self._get_month_name(match.group(2))} {match.group(3)}."
        return "Ismeretlen d√°tum"
    
    def _get_month_name(self, month_str):
        months = ["", "janu√°r", "febru√°r", "m√°rcius", "√°prilis", "m√°jus", "j√∫nius", 
                 "j√∫lius", "augusztus", "szeptember", "okt√≥ber", "november", "december"]
        try:
            m = int(month_str)
            if 1 <= m <= 12:
                return months[m]
        except:
            pass
        return month_str

    async def _async_load_files(self):
        """F√°jlok bet√∂lt√©se."""
        def load():
            history = []
            checked = []
            all_winners = {}
            
            if os.path.exists(self._history_file):
                try: 
                    with open(self._history_file, 'r') as f: history = json.load(f)
                except: pass
            
            if os.path.exists(self._state_file):
                try:
                    with open(self._state_file, 'r') as f: 
                        state = json.load(f)
                        checked = state.get("checked_pdfs", [])
                except: pass

            if os.path.exists(self._all_winners_file):
                try:
                    with open(self._all_winners_file, 'r') as f:
                        all_winners = json.load(f)
                except: pass
                
            return history, checked, all_winners

        self._history, self._checked_pdfs, self._all_winners = await self.hass.async_add_executor_job(load)

    async def _async_save_files(self):
        """F√°jlok ment√©se."""
        def save():
            with open(self._history_file, 'w') as f: json.dump(self._history, f, indent=2)
            with open(self._state_file, 'w') as f: json.dump({"checked_pdfs": self._checked_pdfs}, f)
            with open(self._all_winners_file, 'w') as f: json.dump(self._all_winners, f, indent=2)

        await self.hass.async_add_executor_job(save)

    async def _scan_historical_pdfs(self, session, html_content):
        """V√©gign√©zi az √∂sszes el√©rhet≈ë PDF-et √©s elmenti a nyerteseket."""
        _LOGGER.info("T√∂rt√©nelmi sorsol√°sok vizsg√°lata...")
        pdf_urls = self._extract_pdf_urls_from_html(html_content)
        
        changes_made = False

        for url in pdf_urls:
            date_match = re.search(r'GK_(\d{8})', url)
            if not date_match: continue
            
            date_key = date_match.group(1)
            
            # Ha m√°r megvan √©s van benne adat, kihagyjuk
            if date_key in self._all_winners and self._all_winners[date_key].get("numbers"):
                continue
                
            _LOGGER.debug(f"Feldolgoz√°s: {url}")
            text = await self._extract_text_from_pdf(session, url)
            
            if text:
                all_raw_winners = []
                lines = text.split('\n')
                for line in lines:
                    line = line.strip()
                    if not line: continue
                    # Keres√©s: sz√°m (5 vagy 6 kezdettel, 9 sz√°mjegy)
                    match = re.search(r'\b([56]\d)\s?(\d{7})\b', line)
                    if match:
                        full_num = f"{match.group(1)}{match.group(2)}"
                        car_part = line[match.end():].strip()
                        # Tiszt√≠t√°s
                        car_part = re.sub(r'^\s*[-‚Äì]\s*', '', car_part)
                        car_part = re.sub(r'\s+', ' ', car_part)
                        
                        entry = {"szam": full_num}
                        if car_part and len(car_part) > 3:
                            entry["auto"] = car_part
                        all_raw_winners.append(entry)
                
                date_text = self._parse_date_from_pdf_url(url)
                
                self._all_winners[date_key] = {
                    "text": date_text,
                    "url": url,
                    "scan_date": datetime.now().isoformat(),
                    "numbers": all_raw_winners
                }
                changes_made = True
                _LOGGER.info(f"Sorsol√°s ({date_text}) feldolgozva: {len(all_raw_winners)} nyertes.")
        
        if changes_made:
            await self._async_save_files()
            # Ha v√°ltozott az adatb√°zis, ellen≈ërizni kell a saj√°t sz√°mokat
            self._check_numbers_against_cache()

    def _check_numbers_against_cache(self):
        """√ñsszeveti a saj√°t sz√°mokat a teljes adatb√°zissal."""
        new_win = False
        for date_key, data in self._all_winners.items():
            for winner in data.get("numbers", []):
                if winner["szam"] in self.my_numbers:
                    # Tal√°lat - ellen≈ërizz√ºk, hogy m√°r nincs-e benne (sz√°m + d√°tum alapj√°n, mert lehet t√∂bbsz√∂r nyerni)
                    exists = any(h["szam"] == winner["szam"] and h["datum"] == data["text"] for h in self._history)
                    if not exists:
                        _LOGGER.warning(f"NYEREM√âNY TAL√ÅLAT! {winner['szam']} - {data['text']}")
                        self._history.append({
                            "datum": data["text"],
                            "szam": winner["szam"],
                            "auto": winner.get("auto", "Ismeretlen t√≠pus"),
                            "forras": "El≈ëzm√©nyek"
                        })
                        new_win = True
                        
                        # √ârtes√≠t√©s k√ºld√©se
                        persistent_notification.create(
                            self.hass, 
                            f"Gratul√°lunk! A {winner['szam']} bet√©tk√∂nyv nyert!\nNyerem√©ny: {winner.get('auto', 'Aut√≥')}\nSorsol√°s: {data['text']}",
                            title="üöó OTP G√©pkocsinyerem√©ny",
                            notification_id=f"otp_win_{winner['szam']}"
                        )

        if new_win:
             self.hass.async_create_task(self._async_save_files())
    
    async def _async_update_data(self):
        """Adatok friss√≠t√©se."""
        if not self._all_winners:
            await self._async_load_files()
        
        # El≈ësz√∂r n√©zz√ºk meg a cache-b≈ël (h√°tha √∫j sz√°mot adott hozz√° a user)
        self._check_numbers_against_cache()
        
        try:
            async with async_timeout.timeout(180):
                async with aiohttp.ClientSession() as session:
                    async with session.get(URL) as response:
                        html_content = await response.text()
                    
                    # Aktu√°lis d√°tumok keres√©se az oldalon
                    next_draw = "Ismeretlen"
                    last_draw = "Ismeretlen"
                    
                    nd_match = re.search(r'K√∂vetkez≈ë sorsol√°s:.*?(\d{4}\.\s*\w+\s*\d+\.)', html_content)
                    if nd_match: next_draw = nd_match.group(1)
                    
                    ld_match = re.search(r'Legut√≥bbi sorsol√°s:.*?(\d{4}\.\s*\w+\s*\d+\.)', html_content)
                    if ld_match: last_draw = ld_match.group(1)

                    # Parse current drawing winners from HTML (latest drawing shows on page, not PDF)
                    html_winners = re.findall(r'\b([56]\d)\s?(\d{7})\b', html_content)
                    if html_winners and last_draw != "Ismeretlen":
                        # Extract date key from last_draw (e.g. "2026. janu√°r 15." -> "20260115")
                        date_match = re.search(r'(\d{4})\.\s*(\w+)\s*(\d+)', last_draw)
                        if date_match:
                            year = date_match.group(1)
                            month_name = date_match.group(2).lower()
                            day = date_match.group(3).zfill(2)
                            months = {"janu√°r": "01", "febru√°r": "02", "m√°rcius": "03", "√°prilis": "04",
                                      "m√°jus": "05", "j√∫nius": "06", "j√∫lius": "07", "augusztus": "08",
                                      "szeptember": "09", "okt√≥ber": "10", "november": "11", "december": "12"}
                            month = months.get(month_name, "01")
                            draw_key = f"{year}{month}{day}"
                            
                            if draw_key not in self._all_winners or not self._all_winners[draw_key].get("numbers"):
                                current_winners = []
                                seen_nums = set()
                                for match in html_winners:
                                    num = f"{match[0]}{match[1]}"
                                    if num not in seen_nums and num.startswith(('5', '6')):
                                        seen_nums.add(num)
                                        current_winners.append({"szam": num})
                                if current_winners:
                                    self._all_winners[draw_key] = {
                                        "text": last_draw,
                                        "url": "HTML",
                                        "scan_date": datetime.now().isoformat(),
                                        "numbers": current_winners
                                    }
                                    _LOGGER.info(f"HTML-b≈ël kinyerve {len(current_winners)} nyertes sz√°m ({last_draw})")
                                    await self._async_save_files()
                                    self._check_numbers_against_cache()

                    # T√∂rt√©nelmi PDF-ek szkennel√©se
                    await self._scan_historical_pdfs(session, html_content)

            # Adatok √∂ssze√°ll√≠t√°sa
            self._history.sort(key=lambda x: x.get("datum", ""), reverse=True)
            
            return {
                "nyeremenyek": len(self._history),
                "nyertes_reszletek": self._history,
                "utolso_sorsolas": last_draw,
                "kovetkezo_sorsolas": next_draw,
                "nyeremeny_tortenelem": self._history,
                "figyelt_db": len(self.my_numbers)
            }

        except Exception as err:
            _LOGGER.error(f"Hiba az OTP adatok lek√©r√©sekor: {err}")
            return {
                "nyeremenyek": len(self._history),
                "nyertes_reszletek": self._history,
                "utolso_sorsolas": "Hiba a lek√©rdez√©sben",
                "kovetkezo_sorsolas": "Ismeretlen",
                "nyeremeny_tortenelem": self._history,
                "figyelt_db": len(self.my_numbers)
            }
